{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Prices\n",
    "\n",
    "This project was initially written as a submission for the \"Housing Prices Competition for Kaggle Learn Users\" (https://www.kaggle.com/competitions/home-data-for-ml-course/overview). A description of competition is given below.\n",
    "\n",
    "Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
    "\n",
    "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n",
    "\n",
    "The data was provided in a form of two CSV files - train.csv (containing training data with information about the price of the house) and test.csv (containing testing without information about the price of the house). In the following analysis only the train.csv file will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./dataset/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data describe 1460 houses with 81 features (both numerical and object type). There are some missing values in this dataframe. The prediction target is in column 'SalePrice'. A detailed description of the individual parameters describing the houses can be found on the competition website, under 'data' (https://www.kaggle.com/competitions/home-data-for-ml-course/data). \n",
    "\n",
    "Most of the data in columns 'Alley', 'FireplaceQu', 'PoolQC', 'Fence' and 'MiscFeature' are missing so they will not be taken into account in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage     259\n",
       "MasVnrType        8\n",
       "MasVnrArea        8\n",
       "BsmtQual         37\n",
       "BsmtCond         37\n",
       "BsmtExposure     38\n",
       "BsmtFinType1     37\n",
       "BsmtFinType2     38\n",
       "Electrical        1\n",
       "GarageType       81\n",
       "GarageYrBlt      81\n",
       "GarageFinish     81\n",
       "GarageQual       81\n",
       "GarageCond       81\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns with significant proportion of missing data\n",
    "data.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1, inplace = True)\n",
    "\n",
    "# Print remaining columns with missing data and number of missing data\n",
    "data.isnull().sum()[data.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remaining columns missing data are not the majority, they can be used in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select prediction target (y) and features (X)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "# Divide into training and validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In further analysis two different approaches will be applied. In first approach columns with missing data will be dropped and in the second approach values will be imputed with various strategies. Modelling will be performed with RandomForestRegressor. The score_df function defined below will be used to assess the effectiveness of individual data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_df(X_train, X_val, y_train, y_val):\n",
    "    '''\n",
    "    Function that measure the accuracy of model by mean absolute error\n",
    "    '''\n",
    "    model = RandomForestRegressor(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    mae = mean_absolute_error(y_val, pred)    \n",
    "    \n",
    "    print('R2 value: ', r2)\n",
    "    print('Mean absolute error: ', mae, ' dollars')\n",
    "    return [mae, r2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cardinality of columns with categorical data will be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Street', 2),\n",
       " ('Utilities', 2),\n",
       " ('CentralAir', 2),\n",
       " ('LandSlope', 3),\n",
       " ('GarageFinish', 3),\n",
       " ('PavedDrive', 3),\n",
       " ('LotShape', 4),\n",
       " ('LandContour', 4),\n",
       " ('MasVnrType', 4),\n",
       " ('ExterQual', 4),\n",
       " ('BsmtQual', 4),\n",
       " ('BsmtCond', 4),\n",
       " ('BsmtExposure', 4),\n",
       " ('KitchenQual', 4),\n",
       " ('MSZoning', 5),\n",
       " ('LotConfig', 5),\n",
       " ('BldgType', 5),\n",
       " ('ExterCond', 5),\n",
       " ('HeatingQC', 5),\n",
       " ('Electrical', 5),\n",
       " ('GarageQual', 5),\n",
       " ('GarageCond', 5),\n",
       " ('Condition2', 6),\n",
       " ('RoofStyle', 6),\n",
       " ('Foundation', 6),\n",
       " ('BsmtFinType1', 6),\n",
       " ('BsmtFinType2', 6),\n",
       " ('Heating', 6),\n",
       " ('Functional', 6),\n",
       " ('GarageType', 6),\n",
       " ('SaleCondition', 6),\n",
       " ('RoofMatl', 7),\n",
       " ('HouseStyle', 8),\n",
       " ('Condition1', 9),\n",
       " ('SaleType', 9),\n",
       " ('Exterior1st', 15),\n",
       " ('Exterior2nd', 16),\n",
       " ('Neighborhood', 25)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical columns in the training dataset\n",
    "obj_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "\n",
    "# Get number of unique entries in each column with categorical data\n",
    "obj_nunique = list(map(lambda col: X_train[col].nunique(), obj_cols))\n",
    "obj_dict = dict(zip(obj_cols, obj_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(obj_dict.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the dataset in reasonable size, only columns with cardinality less than 10 will be investigated and encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical columns with cardinality < 10 \n",
    "object_cols = [col for col in obj_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "selected_cols = object_cols + numerical_cols\n",
    "\n",
    "# Keep only selected columns\n",
    "X_train = X_train[selected_cols]\n",
    "X_val = X_val[selected_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. $1^{st}$ approach\n",
    "\n",
    "<b> Dropping columns with missing values </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MasVnrType',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'Electrical',\n",
       " 'GarageType',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond',\n",
       " 'LotFrontage',\n",
       " 'MasVnrArea',\n",
       " 'GarageYrBlt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names of columns with missing values\n",
    "col_miss = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "col_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset before: (1168, 72)  and after the removal of columns with missing data (1168, 58)\n",
      "Shape of training dataset before: (292, 72)  and after the removal of columns with missing data (292, 58)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with missing values in training and validation datasets\n",
    "X_train_red = X_train.drop(col_miss, axis = 1)\n",
    "X_val_red = X_val.drop(col_miss, axis = 1)\n",
    "\n",
    "print('Shape of training dataset before:', X_train.shape,\n",
    "      ' and after the removal of columns with missing data', X_train_red.shape)\n",
    "print('Shape of training dataset before:', X_val.shape,\n",
    "      ' and after the removal of columns with missing data', X_val_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with categorical data:  ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "Columns with numerical data:  ['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n"
     ]
    }
   ],
   "source": [
    "# Select columns with categorical data after reduction\n",
    "obj_red = [col for col in X_train_red.columns if X_train_red[col].dtype == 'object']\n",
    "print('Columns with categorical data: ',obj_red)\n",
    "\n",
    "# Select columns with numerical data after reduction\n",
    "num_red = [col for col in X_train_red.columns if X_train_red[col].dtype in ['int64', 'float64']]\n",
    "print('\\nColumns with numerical data: ', num_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to lack of data, 14 columns were removed from the analyzed set. Next, columns with categorical values will be encoded using OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_encoder = OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n",
    "\n",
    "# Encode categorical data\n",
    "OH_train_red = pd.DataFrame(OH_encoder.fit_transform(X_train_red[obj_red]))\n",
    "OH_val_red = pd.DataFrame(OH_encoder.transform(X_val_red[obj_red]))\n",
    "\n",
    "# Reset indexes\n",
    "OH_train_red.index = X_train_red.index\n",
    "OH_val_red.index = X_val_red.index\n",
    "\n",
    "# Select numerical data\n",
    "num_train_red = X_train_red[num_red]\n",
    "num_val_red = X_val_red[num_red]\n",
    "\n",
    "# Merge columns with numerical and categorical data\n",
    "OH_X_train_red = pd.concat([num_train_red, OH_train_red], axis = 1)\n",
    "OH_X_val_red = pd.concat([num_val_red, OH_val_red], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 value:  0.8482606090509224\n",
      "Mean absolute error:  17514.224246575344  dollars\n"
     ]
    }
   ],
   "source": [
    "# Test first approach\n",
    "score_1 = score_df(OH_X_train_red, OH_X_val_red, y_train, y_val)\n",
    "mae_1 = score_1[0]\n",
    "r2_1 = score_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (MAE) for first approach (dropping columns with missing values) was  17514.224246575344  dollars.\n",
      "R2 value for first approach (dropping columns with missing values) was  0.8482606090509224\n"
     ]
    }
   ],
   "source": [
    "print('Mean absolute error (MAE) for first approach (dropping columns with missing values) was ', mae_1, ' dollars.')\n",
    "print('R2 value for first approach (dropping columns with missing values) was ', r2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with MAE values for different approaches\n",
    "mae_dict = {'Dropping columns with missing values' : mae_1}\n",
    "\n",
    "# Create dictionary with R2 values for different approaches\n",
    "r2_dict = {'Dropping columns with missing values' : r2_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. $2^{nd}$ approach\n",
    "\n",
    "<b> Imputation of missing values</b>\n",
    "\n",
    "Missing numerical values will be imputed using SimpleImputer with 3 different strategies - mean, constant and most_frequent. All of them seems reasonable with the data that are missing.\n",
    "We can assume, that lacks in for example column 'LotFrontage', which stands for linear feet of street connected to property might be caused by:\n",
    "- lack of this information (here, we probably shoud replace it with mean value of other data in this column or assumme, that there is some typical linear feet of street connected to property in this area and replace it with the most common value),\n",
    "- the fact that there is no linear feet of street connected to property and then we probably should replace this values with 0 (constant)\n",
    "\n",
    "All missing categorical data will be replaced with the most frequent value in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "# Imput categorical missing data\n",
    "X_train_cat_imp = pd.DataFrame(cat_imputer.fit_transform(X_train[object_cols]))\n",
    "X_val_cat_imp = pd.DataFrame(cat_imputer.transform(X_val[object_cols]))\n",
    "\n",
    "# Put back removed column names\n",
    "X_train_cat_imp.columns = X_train[object_cols].columns\n",
    "X_val_cat_imp.columns = X_val[object_cols].columns\n",
    "\n",
    "# Encode categorical data\n",
    "OH_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train_cat_imp[object_cols]))\n",
    "OH_X_val = pd.DataFrame(OH_encoder.transform(X_val_cat_imp[object_cols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Numerical missing data imputation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "R2 value:  0.8306449871710455\n",
      "Mean absolute error:  17779.75917808219  dollars\n",
      "\n",
      "\n",
      "most_frequent\n",
      "R2 value:  0.8245125041079162\n",
      "Mean absolute error:  17970.68174657534  dollars\n",
      "\n",
      "\n",
      "constant\n",
      "R2 value:  0.8261279676568541\n",
      "Mean absolute error:  17745.829520547948  dollars\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test different strategies for SimpleImputer\n",
    "for strategy in ['mean', 'most_frequent', 'constant']:\n",
    "    num_imputer = SimpleImputer(strategy = strategy)\n",
    "    \n",
    "    print(strategy)\n",
    "    \n",
    "    # Imput numerical missing data\n",
    "    X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train[numerical_cols]))\n",
    "    X_val_num = pd.DataFrame(num_imputer.transform(X_val[numerical_cols]))\n",
    "    \n",
    "    # Put back removed column names\n",
    "    X_train_num.columns = X_train[numerical_cols].columns\n",
    "    X_val_num.columns = X_val[numerical_cols].columns\n",
    "    \n",
    "    # Merge columns with numerical and categorical data\n",
    "    X_train_imp = pd.concat([X_train_num, OH_X_train], axis = 1)\n",
    "    X_val_imp = pd.concat([X_val_num, OH_X_val], axis = 1)\n",
    "    \n",
    "    # Test second approach with imputation by mean value\n",
    "    score_strategy = score_df(X_train_imp, X_val_imp, y_train, y_val)\n",
    "    print('\\n')\n",
    "    \n",
    "    mae_name = 'Imputation - most_frequent (cat) and '+str(strategy)+' (num)'\n",
    "    mae = score_strategy[0]\n",
    "    \n",
    "    r2_name = 'Imputation - most_frequent (cat) and '+str(strategy)+' (num)'\n",
    "    r2 = score_strategy[1]\n",
    "    \n",
    "    # Add MAE result to the dictionary\n",
    "    mae_dict[mae_name] = mae\n",
    "    \n",
    "    # Add R2 result to the dictionary\n",
    "    r2_dict[r2_name] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion - data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) values for tested approaches:\n",
      "\n",
      "{'Dropping columns with missing values': 17514.224246575344,\n",
      " 'Imputation - most_frequent (cat) and constant (num)': 17745.829520547948,\n",
      " 'Imputation - most_frequent (cat) and mean (num)': 17779.75917808219,\n",
      " 'Imputation - most_frequent (cat) and most_frequent (num)': 17970.68174657534}\n",
      "\n",
      "R2 values for tested approaches:\n",
      "\n",
      "{'Dropping columns with missing values': 0.8482606090509224,\n",
      " 'Imputation - most_frequent (cat) and constant (num)': 0.8261279676568541,\n",
      " 'Imputation - most_frequent (cat) and mean (num)': 0.8306449871710455,\n",
      " 'Imputation - most_frequent (cat) and most_frequent (num)': 0.8245125041079162}\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error (MAE) values for tested approaches:\\n')\n",
    "pprint(mae_dict)\n",
    "print('\\nR2 values for tested approaches:\\n')\n",
    "pprint(r2_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest value of mean absolute error was obtained by the approach based on dropping columns with missing values. However, this approach may lead to the loss of potentially important information for the model and the difference between dropping columns and replacing missing values with a constant (0) is not very large. \n",
    "\n",
    "In data from second approach R2 values for strategy = 'mean' and 'constant' are similar, while the values obtained for 'constant' are slightly lower. However this difference is rather low.\n",
    "\n",
    "Considering this, in further analysis second approach, with strategy = 'constant' will be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for further testing\n",
    "num_imputer = SimpleImputer(strategy = 'constant')\n",
    "   \n",
    "# Imput numerical missing data\n",
    "X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train[numerical_cols]))\n",
    "X_val_num = pd.DataFrame(num_imputer.transform(X_val[numerical_cols]))\n",
    "    \n",
    "# Put back removed column names\n",
    "X_train_num.columns = X_train[numerical_cols].columns\n",
    "X_val_num.columns = X_val[numerical_cols].columns\n",
    "    \n",
    "# Merge columns with numerical and categorical data\n",
    "X_train = pd.concat([X_train_num, OH_X_train], axis = 1)\n",
    "X_val = pd.concat([X_val_num, OH_X_val], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Hyperparameter tuning\n",
    "\n",
    "<b> RandomizedSearchCV </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use: \n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "print('Parameters currently in use: ')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gnerally, most important parameters are:\n",
    "- n_estimators - number of trees in the forest\n",
    "- max_depth - max number of levels in each decision tree\n",
    "- min_samples_split - min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf - min number of data points allowed in a leaf node\n",
    "- max_features - max number of features considered for splitting a node\n",
    "- bootstrap - method for sampling data points (with or without replacement)\n",
    "\n",
    "and they will be tuned.\n",
    "\n",
    "Hyperparameter tuning will be performed with the use of RandomizedSearchCV method.\n",
    "\n",
    "To use RandomizedSearchCV at first there is a need to prepare a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt', 'log2'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [100,\n",
      "                  200,\n",
      "                  300,\n",
      "                  400,\n",
      "                  500,\n",
      "                  600,\n",
      "                  700,\n",
      "                  800,\n",
      "                  900,\n",
      "                  1000,\n",
      "                  1100,\n",
      "                  1200,\n",
      "                  1300,\n",
      "                  1400,\n",
      "                  1500,\n",
      "                  1600,\n",
      "                  1700,\n",
      "                  1800,\n",
      "                  1900,\n",
      "                  2000]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)]\n",
    "\n",
    "# Maximum number of levels in each decision tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 24.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   ccp_alpha=0.0,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   max_samples=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=100,\n",
       "                                                   n_jobs=None, oob_score=Fals...\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000, 1100, 1200,\n",
       "                                                         1300, 1400, 1500, 1600,\n",
       "                                                         1700, 1800, 1900,\n",
       "                                                         2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for best hyperparameters using RandomizedSearchCV (100 combinations, 3 fold cross validation)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1200,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 40,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model performance\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the parameters of the best model obtained using RandomizedSearchCV to the rf_random_best\n",
    "rf_random_best = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    Function for evaluation of model performance\n",
    "    '''\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    \n",
    "    print('Model Performance:')\n",
    "    print('R2 value: ', r2)\n",
    "    print('Mean absolute error: ', mae, ' dollars')\n",
    "    \n",
    "    return [r2, mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic model:\n",
      "Model Performance:\n",
      "R2 value:  0.833543543019414\n",
      "Mean absolute error:  17559.388767123288  dollars\n",
      "\n",
      "Model after RandomizedSearchCV:\n",
      "Model Performance:\n",
      "R2 value:  0.8428845375644615\n",
      "Mean absolute error:  17199.963907914764  dollars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8428845375644615, 17199.963907914764]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic model evaluation\n",
    "print('Basic model:')\n",
    "evaluate(rf, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluation of model after RandomizedSearchCV\n",
    "print('\\nModel after RandomizedSearchCV:')\n",
    "evaluate(rf_random_best, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the RandomizedSearchCV efficiency of the model is improved. Mean absolute error is smaller by about 27.06 dollars and R2 value is greater by about 0.0205.\n",
    "\n",
    "Further improvement of Random Forest model will be implemented using the GridSearchCV method. Grid of parameters to analyze will be determined basing on the results from RandomizedSearchCV.\n",
    "\n",
    "<b>GridSearchCV</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [False],\n",
      " 'max_depth': [35, 40, 45, 50],\n",
      " 'min_samples_leaf': [1, 2, 3, 4],\n",
      " 'min_samples_split': [2, 3, 4],\n",
      " 'n_estimators': [1150, 1200, 1250, 1300, 1350, 1400]}\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of RandimizedSearchCV\n",
    "param_grid = {'n_estimators': [1150, 1200, 1250, 1300, 1350, 1400],\n",
    "               'max_depth': [35, 40, 45, 50],\n",
    "               'min_samples_split': [2, 3, 4],\n",
    "               'min_samples_leaf': [1, 2, 3, 4],\n",
    "               'bootstrap': [False]}\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 30.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                             criterion='mse', max_depth=None,\n",
       "                                             max_features='sqrt',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             max_samples=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             oob_score=False, random_state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [False], 'max_depth': [35, 40, 45, 50],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4],\n",
       "                         'min_samples_split': [2, 3, 4],\n",
       "                         'n_estimators': [1150, 1200, 1250, 1300, 1350, 1400]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model, parameter 'max_features' will be set as 'sqrt'\n",
    "rf_1 = RandomForestRegressor(max_features = 'sqrt')\n",
    "\n",
    "# Search for best hyperparameters using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = rf_1, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 40,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1150}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the parameters of the best model obtained using RandomizedSearchCV to the rf_random_best\n",
    "rf_grid_best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after RandomizedSearchCV:\n",
      "Model Performance:\n",
      "R2 value:  0.8451789333610115\n",
      "Mean absolute error:  17163.150550799084  dollars\n",
      "\n",
      "Basic model (for GridSearchCV):\n",
      "Model Performance:\n",
      "R2 value:  0.8276722128541301\n",
      "Mean absolute error:  18247.79869863014  dollars\n",
      "\n",
      "Model after GridSearchCV:\n",
      "Model Performance:\n",
      "R2 value:  0.8434768872894878\n",
      "Mean absolute error:  17251.2957176891  dollars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8434768872894878, 17251.2957176891]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation of model after RandomizedSearchCV\n",
    "print('Model after RandomizedSearchCV:')\n",
    "evaluate(rf_random_best, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Basic model (for GridSearchCV) evaluation\n",
    "print('\\nBasic model (for GridSearchCV):')\n",
    "evaluate(rf_1, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluation of model after GridSearchCV\n",
    "print('\\nModel after GridSearchCV:')\n",
    "evaluate(rf_grid_best, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model after tuning hyperparameters with use of GridSearchCV had slightly worse performance than the model after applying RandomizedSearchCV. Considering this, parameters determined by the RandomizedSearchCV will be applied into a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RandomForestRegressor(n_estimators = 1200,\n",
    "                                    min_samples_split = 2,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    max_features = 'sqrt',\n",
    "                                    max_depth = 40,\n",
    "                                    bootstrap = False, \n",
    "                                    random_state = 0\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model:\n",
      "Model Performance:\n",
      "R2 value:  0.8465803190620228\n",
      "Mean absolute error:  17181.76764649924  dollars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8465803190620228, 17181.76764649924]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation of final model\n",
    "print('Final model:')\n",
    "evaluate(final_model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Wrap it up! (into a pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ABsolute Error (MAE) for final model is: 17181.76764649924  dollars\n",
      "R2 value for final model is: 0.8465803190620228\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# Drop columns with significant proportion of missing data\n",
    "data.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1, inplace = True)\n",
    "\n",
    "# Select prediction target (y) and features (X)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "# Divide into training and validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Select categorical columns with cardinality < 10 \n",
    "object_cols = [col for col in obj_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "selected_cols = object_cols + numerical_cols\n",
    "\n",
    "# Keep only selected columns\n",
    "X_train = X_train[selected_cols]\n",
    "X_val = X_val[selected_cols]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy = 'constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    ('inputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "]) \n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, object_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "final_model = RandomForestRegressor(n_estimators = 1200,\n",
    "                                    min_samples_split = 2,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    max_features = 'sqrt',\n",
    "                                    max_depth = 40,\n",
    "                                    bootstrap = False, \n",
    "                                    random_state = 0\n",
    "                                   )\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', final_model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training dataset, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation dataset, get predictions\n",
    "pred = my_pipeline.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean ABsolute Error (MAE) for final model is:', mean_absolute_error(y_val, pred),' dollars')\n",
    "print('R2 value for final model is:', r2_score(y_val, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
